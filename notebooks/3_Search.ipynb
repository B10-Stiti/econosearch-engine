{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8062d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1) IMPORT LIBRARIES + LOAD DATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load cleaned docs (optional, but useful for display)\n",
    "with open(\"cleaned_docs.pkl\", \"rb\") as f:\n",
    "    cleaned_docs = pickle.load(f)\n",
    "\n",
    "# Load original file IDs\n",
    "import json\n",
    "with open(\"file_ids.json\", \"r\") as f:\n",
    "    file_ids = json.load(f)\n",
    "\n",
    "\n",
    "# Load fitted TF-IDF vectorizer\n",
    "with open(\"vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "# Load TF-IDF matrix (10788 documents × ~5000 features)\n",
    "from scipy import sparse\n",
    "\n",
    "X = sparse.load_npz(\"tfidf_matrix.npz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d964a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2) FUNCTION TO PREPROCESS A QUERY\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_query(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize a user query so it matches the cleaned docs.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    cleaned = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(cleaned)   # MUST return string for TF-IDF vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f258d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08814744 0.         0.         ... 0.         0.         0.        ]\n",
      "[ 4231  3956  4021  4243  1257  3504  6267  4181 10304  1208]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4231,  3956,  4021,  4243,  1257,  3504,  6267,  4181, 10304,\n",
       "         1208]),\n",
       " array([0.42434699, 0.349468  , 0.34057511, 0.31831363, 0.19643211,\n",
       "        0.19217876, 0.18875948, 0.16738737, 0.16139287, 0.14684295]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3) RANK DOCUMENTS USING COSINE SIMILARITY\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def search(query, k=10):\n",
    "    \"\"\"\n",
    "    Run a query on the TF-IDF matrix and get top-k results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned_q = preprocess_query(query)\n",
    "\n",
    "    # Convert query to TF-IDF vector\n",
    "    q_vec = vectorizer.transform([cleaned_q])\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    sims = cosine_similarity(q_vec, X).ravel()\n",
    "\n",
    "    # Get top-k document indices\n",
    "    top_idx = sims.argsort()[::-1][:k]\n",
    "    return top_idx, sims[top_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42586d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.06921612 ... 0.         0.         0.        ]\n",
      "[6126 4717 8784 1740 3750]\n",
      "Rank 1 — Doc: training/2775 — Score: 0.4978\n",
      "Preview: c r u d e   o i l   p r i c e   s t o c k   o u t p u t   f a l l   u s   c r u\n",
      "------------------------------------------------------------\n",
      "Rank 2 — Doc: training/127 — Score: 0.4869\n",
      "Preview: d i a m o n d   s h a m r o c k   d i a   c u t   c r u d e   p r i c e   d i a\n",
      "------------------------------------------------------------\n",
      "Rank 3 — Doc: training/6876 — Score: 0.4832\n",
      "Preview: d i v i s   s e e n   h e l p   u s   o i l   i n d u s t r i   u s   c o n g r\n",
      "------------------------------------------------------------\n",
      "Rank 4 — Doc: test/18746 — Score: 0.4800\n",
      "Preview: u n i o n   p a c i f   l t u n p   r a i s   c r u d e   o i l   p r i c e   u\n",
      "------------------------------------------------------------\n",
      "Rank 5 — Doc: training/11149 — Score: 0.4423\n",
      "Preview: u s   r e a s s e s s   m i d e a s t   p o l i c i   a n a l y s t   u s   r e\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4) TEST SEARCH ENGINE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "query = \"oil prices\"\n",
    "top_docs, scores = search(query, k=5)\n",
    "\n",
    "for rank, (doc_id, score) in enumerate(zip(top_docs, scores)):\n",
    "    print(f\"Rank {rank+1} — Doc: {file_ids[doc_id]} — Score: {score:.4f}\")\n",
    "    \n",
    "    # preview first 40 tokens\n",
    "    print(\"Preview:\", \" \".join(cleaned_docs[doc_id][:40]))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8002fec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "QUERY: OIL PRICES\n",
      "==============================\n",
      "[0.         0.         0.06921612 ... 0.         0.         0.        ]\n",
      "[6126 4717 8784 1740 3750]\n",
      "1. training/2775 — score: 0.4978\n",
      "2. training/127 — score: 0.4869\n",
      "3. training/6876 — score: 0.4832\n",
      "4. test/18746 — score: 0.4800\n",
      "5. training/11149 — score: 0.4423\n",
      "\n",
      "==============================\n",
      "QUERY: FOREIGN EXCHANGE\n",
      "==============================\n",
      "[0.03416077 0.         0.         ... 0.         0.         0.        ]\n",
      "[7741 1447 7672 4636 8133]\n",
      "1. training/5279 — score: 0.5358\n",
      "2. test/17930 — score: 0.4785\n",
      "3. training/5181 — score: 0.4740\n",
      "4. training/12480 — score: 0.4421\n",
      "5. training/5841 — score: 0.3899\n",
      "\n",
      "==============================\n",
      "QUERY: COMPANY EARNINGS\n",
      "==============================\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[8559 4351 4119 7965 4076]\n",
      "1. training/6539 — score: 0.5500\n",
      "2. training/12050 — score: 0.5034\n",
      "3. training/11708 — score: 0.4918\n",
      "4. training/5579 — score: 0.4814\n",
      "5. training/11637 — score: 0.4614\n",
      "\n",
      "==============================\n",
      "QUERY: GRAIN EXPORTS\n",
      "==============================\n",
      "[0.14591123 0.15462384 0.         ... 0.         0.         0.        ]\n",
      "[4976 4162 8111 9606 9463]\n",
      "1. training/13173 — score: 0.4949\n",
      "2. training/11769 — score: 0.4827\n",
      "3. training/5800 — score: 0.4637\n",
      "4. training/8161 — score: 0.4441\n",
      "5. training/7934 — score: 0.4222\n",
      "\n",
      "==============================\n",
      "QUERY: INTEREST RATES\n",
      "==============================\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1643 4376 4749 4145 9202]\n",
      "1. test/18520 — score: 0.6046\n",
      "2. training/12091 — score: 0.5564\n",
      "3. training/12774 — score: 0.5536\n",
      "4. training/11746 — score: 0.5431\n",
      "5. training/7538 — score: 0.5185\n",
      "\n",
      "==============================\n",
      "QUERY: TRADE BALANCE\n",
      "==============================\n",
      "[0.18275445 0.         0.02281293 ... 0.         0.         0.        ]\n",
      "[6678 5142 6508 5143 5299]\n",
      "1. training/3610 — score: 0.5145\n",
      "2. training/13842 — score: 0.4069\n",
      "3. training/3348 — score: 0.3718\n",
      "4. training/13849 — score: 0.3692\n",
      "5. training/14573 — score: 0.3676\n",
      "\n",
      "==============================\n",
      "QUERY: INFLATION REPORT\n",
      "==============================\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[9580 7818   44 2996 5400]\n",
      "1. training/8119 — score: 0.4032\n",
      "2. training/5375 — score: 0.3885\n",
      "3. test/14918 — score: 0.3815\n",
      "4. test/21535 — score: 0.3747\n",
      "5. training/1550 — score: 0.3700\n",
      "\n",
      "==============================\n",
      "QUERY: MARKET ACQUISITION\n",
      "==============================\n",
      "[0.03452247 0.         0.         ... 0.         0.         0.        ]\n",
      "[ 678  675 9019 1411 5829]\n",
      "1. test/16041 — score: 0.4035\n",
      "2. test/16033 — score: 0.3993\n",
      "3. training/7234 — score: 0.3501\n",
      "4. test/17867 — score: 0.3467\n",
      "5. training/2301 — score: 0.3374\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"oil prices\",\n",
    "    \"foreign exchange\",\n",
    "    \"company earnings\",\n",
    "    \"grain exports\",\n",
    "    \"interest rates\",\n",
    "    \"trade balance\",\n",
    "    \"inflation report\",\n",
    "    \"market acquisition\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"QUERY:\", q.upper())\n",
    "    print(\"==============================\")\n",
    "\n",
    "    top_docs, scores = search(q, 5)\n",
    "\n",
    "    for rank, (doc_id, score) in enumerate(zip(top_docs, scores)):\n",
    "        print(f\"{rank+1}. {file_ids[doc_id]} — score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c4ad26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.06921612 ... 0.         0.         0.        ]\n",
      "[6126 4717 8784 1740 3750 6795 9794 4127 8293 8233]\n",
      "[0.03416077 0.         0.         ... 0.         0.         0.        ]\n",
      "[7741 1447 7672 4636 8133 4393 6425 1705  510 6430]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[8559 4351 4119 7965 4076 6779 7596 5053 3401 4122]\n",
      "[0.14591123 0.15462384 0.         ... 0.         0.         0.        ]\n",
      "[4976 4162 8111 9606 9463  584 3126  493 3695 8206]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1643 4376 4749 4145 9202 1632 1662 4497 5637 1295]\n",
      "[0.18275445 0.         0.02281293 ... 0.         0.         0.        ]\n",
      "[ 6678  5142  6508  5143  5299 10150  1036  3691  6875  6179]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[9580 7818   44 2996 5400 7821 1479 4884 2065 5484]\n",
      "[0.03452247 0.         0.         ... 0.         0.         0.        ]\n",
      "[  678   675  9019  1411  5829  1712  3342 10541  2937  1120]\n",
      "Saved search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6) SAVE RANKING RESULTS FOR EVALUATION NOTEBOOK\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(\"search_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"query\", \"rank\", \"doc_id\", \"score\"])\n",
    "\n",
    "    for q in queries:\n",
    "        top_docs, scores = search(q, 10)\n",
    "        for rank, (doc_id, score) in enumerate(zip(top_docs, scores)):\n",
    "            writer.writerow([q, rank+1, file_ids[doc_id], score])\n",
    "\n",
    "print(\"Saved search_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
